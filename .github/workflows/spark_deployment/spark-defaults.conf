# Basic Spark Configuration
spark.master                                   spark://spark-master:7077

# Memory Configuration for GitHub Actions Runner (16GB RAM)
spark.driver.memory                            4g
spark.executor.memory                          6g
spark.memory.offHeap.enabled                   true
spark.memory.offHeap.size                      2g
spark.memory.fraction                          0.8
spark.memory.storageFraction                   0.3

# JVM Options
spark.driver.extraJavaOptions                  -XX:+UseG1GC -XX:+UseCompressedOops
spark.executor.extraJavaOptions                -XX:+UseG1GC -XX:+UseCompressedOops

# Executor Configuration (4 cores)
spark.executor.cores                           4
spark.executor.instances                       1
spark.default.parallelism                      8
spark.sql.shuffle.partitions                   8

# Performance Optimization
spark.sql.adaptive.enabled                     true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.sql.adaptive.localShuffleReader.enabled  true
spark.sql.adaptive.skewJoin.enabled           true
spark.sql.adaptive.advisoryPartitionSizeInBytes 64m

# Storage Optimization
spark.local.dir                               /tmp/spark-temp
spark.disk.spillSize                          512m
spark.sql.files.maxPartitionBytes             67108864
spark.sql.inMemoryColumnarStorage.compressed   true

# Network and Shuffle Settings
spark.shuffle.compress                         true
spark.shuffle.spill.compress                   true
spark.io.compression.codec                     lz4
spark.io.compression.lz4.blockSize             32k

# Catalog Configuration
spark.sql.catalog.glue                         org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.glue.catalog-impl            org.apache.iceberg.aws.glue.GlueCatalog
spark.sql.catalog.glue.io-impl                 org.apache.iceberg.aws.s3.S3FileIO
spark.sql.defaultCatalog                       glue

# S3 Configuration
spark.hadoop.fs.s3a.impl                       org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.access.key                 ${AWS_ACCESS_KEY_ID}
spark.hadoop.fs.s3a.secret.key                 ${AWS_SECRET_ACCESS_KEY}
spark.hadoop.fs.s3a.endpoint                   s3.${AWS_REGION}.amazonaws.com
spark.hadoop.fs.s3a.path.style.access          true
spark.hadoop.fs.s3a.region                     ${AWS_REGION}

# S3 Performance Optimization
spark.hadoop.fs.s3a.connection.maximum         50
spark.hadoop.fs.s3a.connection.timeout         30000
spark.hadoop.fs.s3a.attempts.maximum           10
spark.hadoop.fs.s3a.connection.establish.timeout 30000
spark.hadoop.fs.s3a.readahead.range           128K
spark.hadoop.fs.s3a.impl.disable.cache        false
spark.hadoop.fs.s3a.buffer.dir                /tmp/s3a

# Development Optimizations
spark.sql.execution.arrow.pyspark.enabled      true
spark.sql.execution.arrow.maxRecordsPerBatch   10000
spark.ui.port                                 4040
spark.ui.retainedJobs                         50
spark.ui.retainedStages                       50
spark.ui.retainedTasks                        50

# Warehouse Configuration
spark.sql.warehouse.dir                        s3a://dbt-spark-iceberg/github-integration-testing
spark.sql.catalog.glue.warehouse               s3a://dbt-spark-iceberg/github-integration-testing/unified
spark.sql.catalog.glue.database                unified

# Thrift Server Settings
spark.sql.hive.thriftServer.singleSession      true
spark.sql.hive.thriftServer.async              true
spark.sql.hive.thriftServer.maxWorkerThreads   4
spark.sql.hive.thriftServer.workerQueue.size   100

# Window Operations
spark.sql.window.exec.buffer.in.memory.threshold 50000
spark.sql.window.exec.buffer.spill.threshold 100000

# Join Optimizations
spark.sql.adaptive.skewJoin.enabled true