# Core Spark Configuration - Constrained for 4 CPU, 16GB RAM machine
spark.master                                   local[4]
spark.driver.memory                           8g      # Half of total RAM
spark.executor.memory                         4g      # Quarter of total RAM
spark.memory.fraction                         0.7     # Fraction of heap space for execution/storage
spark.memory.storageFraction                  0.3     # Fraction of memory fraction for storage
spark.memory.offHeap.enabled                  true
spark.memory.offHeap.size                     2g      # Small off-heap to avoid OOM

# Parallelism and Partitioning - Adjusted for 4 CPU
spark.sql.shuffle.partitions                  8       # 2x number of cores
spark.default.parallelism                     8       # 2x number of cores
spark.sql.files.maxPartitionBytes            134217728  # 128MB per partition

# Performance Optimization
spark.sql.adaptive.enabled                    true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled          true
spark.serializer                             org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max              256m
spark.sql.inMemoryColumnarStorage.compressed true
spark.sql.inMemoryColumnarStorage.batchSize  10000

# S3A Configuration
spark.hadoop.fs.s3a.impl                     org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.access.key               ${AWS_ACCESS_KEY_ID}
spark.hadoop.fs.s3a.secret.key               ${AWS_SECRET_ACCESS_KEY}
spark.hadoop.fs.s3a.session.token            ${AWS_SESSION_TOKEN}
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider
spark.hadoop.fs.s3a.endpoint                 s3.eu-west-1.amazonaws.com
spark.hadoop.fs.s3a.path.style.access        false
spark.hadoop.fs.s3a.region                   eu-west-1

# S3A Connection Management - Conservative settings
spark.hadoop.fs.s3a.connection.maximum       20      # Reduced connection pool
spark.hadoop.fs.s3a.connection.timeout       300000  # 5 minutes
spark.hadoop.fs.s3a.threads.max              8       # 2x cores
spark.hadoop.fs.s3a.connection.ssl.enabled   true
spark.hadoop.fs.s3a.readahead.range         128K    # Reduced readahead
spark.hadoop.fs.s3a.retry.limit             10      # Reasonable retry limit

# Iceberg Catalog Configuration
spark.sql.extensions                         org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.glue                       org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.glue.catalog-impl          org.apache.iceberg.aws.glue.GlueCatalog
spark.sql.catalog.glue.warehouse             s3a://dbt-spark-iceberg/github-integration-testing
spark.sql.catalog.glue.io-impl               org.apache.iceberg.aws.s3.S3FileIO
spark.sql.defaultCatalog                     glue

# Timeouts and Network
spark.network.timeout                        300s    # 5 minutes
spark.sql.broadcastTimeout                   300s    # 5 minutes

# Garbage Collection
spark.executor.extraJavaOptions              -XX:+UseG1GC -XX:G1HeapRegionSize=16M -XX:+UseCompressedOops